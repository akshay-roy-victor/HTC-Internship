# HT Consulting Internship-
This repository contains all my projects done during my internship with HT Consulting

Notebook file contains all the code and explanations/overviews of how the code works.
Data file contains all my results of altering data samples, it is split into audio, text, and image folders. The backup folder contains the original data samples unaltered.
Requirements file contains all the libraries and tools needed to run the code.
The file labeled "Jimmy" contains all assignments he has tasked the interns/new employees with.

Below is a summary of all tasks completed in each week:

## Week 1
- Familiarize and experiment with OS libraries to handle file moving and sorting.
- Familiarize and experiment with pandas, matplotlib, and numpy libraries to learn to plot graphs and convert data into numpy arrays
- Convert audio files into spectrograms with pytorch
- Pandas was used to alter and read csv files.
- Learn to use jupyter notebook for my source code
- Worked on setting up a github repo for all my projects in this company.
- Access and read valuable properties of images such as formatting type, size, pixels, etc.

## Week 2
- Convert more audio files into spectrograms and then convert them into numpy arrays and tensors.
- Learned more in depth into using pytorch and tensorflow libraries to inspect tensors and other additional things to do.
- Beautify my github repo by organizing files and adding a readme file alongside requirements file. 
- Manually scan cheques to help create datasets for another team working on a project.
- We got to see how a cheque scanning machine works on the inside and errors to look out for in these machines.

## Week 3
- During this week I was given multiple large datasets that consisted of images and csv files. For each dataset I was tasked to plot appropriate graphs of certain features of each of this data using matplotlib, seaborn,     and plotly libraries.
- I learned key terms to know when handling big data such as mean, standard deviation, right/left-skewed graphs, etc.
- I was tasked with generating code to separate images based on the labels given in their corresponding csv data file.

## Week 4
- I learned to use libraries such as tensorflow, scikit-learn, and pytorch to get familiar with making and training AI models.
- I was given 2 datasets: one being of images of cats and dogs, another with a csv dataset of all passengers of the titanic. I was tasked to train an AI model to differentiate between cats and dogs for the image dataset     and predict which passengers would survive in the csv dataset. I had to use all 3 different libraries for each dataset
- I learned generally on loading data, preprocessing data, choosing features and prediction target, splitting data into training, validation, and testing segments, structuring/designing an AI model, training the AI model,     evaluating its performance, testing the model on new data, and saving the model.

## Week 5
- I was given tasks that were related to the previous week regarding the AI image classification model ; Generally, I had to plot graphs and tabulate data of the accuracies of my AI model based on their corresponding       number of images used for training the model.
- Graphs discovered shows a positive increase in accuracy of a model when number of images per class for training increased, however there was a significant plateau at a certain point. This was to help familiarize the       optimum number of images needed to train a model such that I avoid overfitting.
- I learned from my peers to use YOLO which is an object detection model so that I can assist them in their work to detect signatures, names, date, etc. on a bank cheque.
- I also researched other AI models such as detecto and roboflow.
- I then had to label and train sets of bank cheque images ranging from 10, 100, 250, 500, and 1000 images per class and evaluate each model’s performance.

## Week 6
- I had to bug fix the YOLO object detection source code as it could not detect my GPU and thus the model training took a long amount of time to train.
- ### JIMMY
  - This was the start of Jimmy’s assignments. He had given us tasks to experiment with bit manipulation and discover the booth algorithm.
  - I had to implement functions to set bits, convert decimals to binary, unset bits, find max number of a short data type, and the booth algorithm.

## Week 7
- During this week, I had to retrain some datasets using the YOLO object detection model as some data was inaccurate/lost. I also had to plot their performance on a graph and tabulate information such as image per class     size, mAP accuracies, and accuracy on random bank cheque image samples, both coloured and grayscale.
- ### JIMMY
  - I had to learn to differentiate the performances of switch-case statements and if-else statements; I had to implement functions that used both methods to give an output of number of days based on the month passed into     the function.
  - I also had to implement a function that converts all letters of a string to uppercase, in recursive manner, and pass by reference.
  - I also had to implement a function that prints all possible combinations of a string given the number of letters, and length of string.

## Week 8
- More bug fixing was needed for YOLO as the model couldn't detect my gpu
- ### JIMMY
  - I had to implement a stack, and then use the stack to modify my to uppercase function from before such that instead of working recursively, it instead works in a while loop utilizing the stack to store the letters of      a string. This function also had to work by passing variables by value.
  - I also had to implement a binary tree alongside tree depth reversal functions.
  - I also had to research on how binary trees worked.

## Week 9
-
